---
title: "Example from Section 2 in the book Contemporary Empirical Methods in Software Engineering"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Richard Torkar, Carlo A. Furia, and Robert Feldt"
date: "4/7/2019"
output: html_document
bibliography: ./refs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prior predictive analysis
We will make use of a number of `R` packages. Both `rethinking` and `brms` allows us to develop models which we then run using `Stan`. In this example we'll see both being used. Make sure that you have the following installed (including <http://www.mc-stan.org>):
```{r packages, message=FALSE}
library(rethinking)
library(brms)
library(tidyverse)
library(ggthemes)
library(extraDistr)
library(ggpubr)
```

Let's start by attaching the data which we will use for our example. This data comes from the `rethinking` package and according to [@mcelreath] consists of:

> partial census data for the Dobe area !Kung San, compiled from interviews conducted by Nancy Howell in the late 1960s.

```{r data}
data(Howell1)
d <- Howell1
```
Let us keep only subjects that are >= 18 years of age.
```{r}
d2 <- d[ d$age >= 18 , ]
```

Sample randomly for our three priors on $\alpha$, $\beta$, and $\sigma$, and plot the results. Additionally, let us plot the combination of all priors to represent the combined effect of the priors on $\mu$.

```{r echo=FALSE, warning=FALSE, fig.align='center'}
p_alpha <- ggplot(data.frame(x = c(100, 250)), aes(x = x)) +
  stat_function(fun = dnorm, args=list(mean=181, sd = 20)) + theme_tufte() +
  xlab(expression(alpha)) +
  ylab("") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

p_beta <- ggplot(data.frame(x = c(-50, 50)), aes(x = x)) +
  stat_function(fun = dnorm, args=list(mean=0, sd = 10)) + theme_tufte() +
  xlab(expression(beta)) +
  ylab("") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

p_cauchy <- ggplot(data.frame(x = c(0, 100)), aes(x = x)) +
  stat_function(fun = dhcauchy, args=list(sigma=10)) + theme_tufte() +
  xlab(expression(sigma)) +
  ylab("") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

sample_alpha <- rnorm(1e4, 181, 20)
sample_sigma <- rhcauchy(1e4, 10)
sample_beta <- rnorm(1e4, 0, 10)
prior_h <- rnorm(1e4, sample_alpha + sample_beta, sample_sigma)
prior_h <- data.frame(length=prior_h)

p_combo <- ggplot(data.frame(prior_h), aes(x=length)) +  geom_density() + xlim(0,360) + 
  xlab(expression(mu, " and ", sigma)) +
  ylab("") +
  theme_tufte() 

ggarrange(ggarrange(p_alpha, p_beta, p_cauchy, ncol=3),
          p_combo, nrow=2)
```

So, above we see each prior and the combination of the priors for $\mu$. In short, we assume $alpha \sim \mathcal{N}(181,20)$, $\beta \sim \mathcal{N}(0,10)$, and $\sigma \sim \text{Half-Cauchy}(0,10)$.

We can now check for sanity issues concerning our priors. For example, how many people are <53 or >272 cm? Well, it's easy to do now when we have a "posterior", i.e., the answer is `r sum(prior_h < 0.53) / 10000 * 100`% and `r 100 - (sum(prior_h > 2.72) /10000 * 100)`%, respectively.

In short, we still allow extreme values, but we don't allow too many absurd values. To summarize, our model now looks like,
\[
\begin{eqnarray}
\text{height}_i & \sim & \mathcal{N}(\mu_i,\sigma)\\
\mu_i & \sim & \alpha + \beta_w \times \text{weight}_i\\
\alpha & \sim & \mathcal{N}(181, 20)\\
\beta_w & \sim & \mathcal{N}(0, 10)\\
\sigma & \sim & \text{Half-Cauchy}(0, 10)
\end{eqnarray}
\]
Let us now go through this line by line. First, we claim that height has a Normal distribution with mean $\mu$ and standard deviation $\sigma$. The subset $i$ in height, weight and $\mu$ is an indication that this holds for each height we have in the data set. But why Normal? Well, there are ontological and epistemological reasons for this, but in short: if we add together random values from the same distribution it converges to a normal distribution.

The next line shows our linear model. We have an intercept $\alpha$, together with the slope $\beta_w$. We want to estimate these two parameters using the data: height and weight. Height is the outcome and weight is the predictor.

Next, we have our priors. The $\alpha$ parameter is the intercept, and hence our expected mean. What we are saying is that we have prior knowledge, i.e., we believe that the mean will gravitate towards 181 cm. Why 181? Well, this is the average height of the three authors of this exercise. In addition, we say that we can expect the mean to vary with a standard deviation of 20. For $\beta_w$ our prior indicates that the slope has a mean of 0 and a standard deviation of 10. We could also set a different prior here, e.g., we have a feeling that an increase in weight also leads to an increase in height, but for now we will leave it at that. Finally, we have a prior on $\sigma$ known as Half-Cauchy. The Half-Cauchy is a common prior for $\sigma$ and is roughly a Normal distribution cut in half, i.e., we do not allow negative values on $\sigma$. In the end, if we have enough evidence (data) it will swamp the priors. (Feel free to mess around in the code and see what happens when you change the priors.)

## Model and execution (`rethinking`)
The below model is declared using the "rethinking" package and as you see it closely follows our model declaration we did in math above. In the examples we show here we have not standardized our predictor values. This is something one, generally speaking, always should do, mainly to help `Stan' sample values.

```{r ulam, echo=TRUE, warning=TRUE, results='hide'}
model <- ulam(
  alist(
    height ~ normal(mu, sigma),
    mu <- alpha + beta_w * weight,
    alpha ~ normal(181, 20),
    beta_w ~ normal(0,10),
    sigma ~ cauchy(0,10)
  ), data = d2, chains = 4, cores = 4, iter = 1e4, control=list(max_treedepth=13)
)
```
After having run the model we can now look at the posterior. Let's use `precis()` from the `rethinking` package, which summarizes the posterior for us.
```{r precis, echo=TRUE}
precis(model)
```
What we see is that the intercept ($\alpha$) is approximately `r round(precis(model)$mean[1], digits=2)` cm (the !Kung people seems to be short), and there's a positive $\beta_w$ parameter, i.e., weight and height goes partly hand in hand. In this particular case we're saying that a person 1 kg heavier is expected to be `r round(precis(model)$mean[2], digits=2)` cm taller

Let's now plot our data and the linear prediction we calculated.
```{r echo=FALSE, warning=FALSE, fig.align='center'}

posterior <- extract.samples(model)
weight.seq <- seq(from=25, to=70, length.out=352)
sim <- sim(model, data=list(weight=weight.seq))
height.PI <- apply( sim , 2 , PI , prob=0.95 )

a <- mean(posterior$alpha)
b <- mean(posterior$beta_w)

ggplot(d2, aes(x=weight, y=height)) + geom_point(color="grey90") + 
  geom_abline(intercept=a, slope=b) + 
  theme_tufte()
```

## Model and execution (`brms`)
The book McElreath [-@mcelreath] wrote is a good introduction to Bayesian data analysis. However, more straightforward tools exist, e.g., `brms` which uses the common `lme4` syntax for declaring models. Below we declare and run the same model in `brms` 
```{r brms, echo=TRUE, warning=FALSE, results='hide'}
model_brms <- brm(bf(height ~ 1 + weight), 
             prior = c(prior(normal(181,20), class = "Intercept"),
                       prior(normal(0,10), class="b"),
                       prior(cauchy(0,10), class="sigma")),
             data=d2, chains=4, cores=4, iter=1e4)
```
With `brms` we can use the `summary()` function to view a summary of the posterior probability distribution. As you see below, these values are very much the same as when we used `rethinking`.
```{r summary, echo=TRUE}
summary(model_brms)
```

Let's now plot the empirical data, but this time we also plot the uncertainty by using $\sigma$. The dark line is the maximum a posteriori estimate of $\mu$. The two dark intervals show two different 95% uncertainty intervals. The dark gray interval shows the uncertainty around $\mu$. The light gray interval shows the uncertainty concerning all values, i.e., 95% of all values should be within this interval.

```{r echo=FALSE, warning=FALSE, fig.align='center'}
weight_seq <- tibble(weight = seq(from = 25, to = 70, by = 1))

mu <-
  fitted(model_brms,
         summary = F,
         newdata = weight_seq) %>%
  as_tibble() %>%
  mutate(Iter = 1:20000) %>%
  select(Iter, everything())

pred_height <-
  predict(model_brms,
          newdata = weight_seq) %>%
  as_tibble() %>%
  bind_cols(weight_seq)

mu_summary <-
  fitted(model_brms, 
         newdata = weight_seq) %>%
  as_tibble() %>%
  bind_cols(weight_seq)

d2 %>%
  ggplot(aes(x = weight)) +
  geom_ribbon(data = pred_height, 
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              fill = "grey85") +
  geom_ribbon(data = mu_summary, 
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              fill = "grey60") +
  geom_line(data = mu_summary, aes(y = Estimate)) +
  geom_point(aes(y = height), 
             shape = 1, size = 1.5, alpha = 2/5) +
  coord_cartesian(xlim = range(d2$weight),
                  ylim = range(d2$height)) +
  theme(panel.grid = element_blank()) + ylab("height") +
  theme_tufte()
```

### References